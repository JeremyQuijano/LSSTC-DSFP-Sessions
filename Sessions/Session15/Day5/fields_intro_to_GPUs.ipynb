{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/carlnotsagan/LSST-DSFP-Session15-Materials/blob/main/fields_intro_to_GPUs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "quHFe1dTJVan"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuAyY7U9JVbK"
   },
   "source": [
    "# Introduction to GPUs (in Python):\n",
    "\n",
    "By Carl Fields (Los Alamos National Lab)\n",
    "\n",
    "*This exercise was designed heavily based on tutorials from the [GTC 2018 Conference](https://github.com/ContinuumIO/gtc2018-numba).* \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHPCMLpBJVbd"
   },
   "source": [
    "## Problem 0) Creating Our HPC Environment\n",
    "**Before** beginning, we want to prepare a new environment for the purpose of this exercise.\n",
    "\n",
    "Be sure to activate our HPC environment from last time:\n",
    "\n",
    "```linux\n",
    "$ conda activate hpc\n",
    "```\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mb4EgY5YJVbb"
   },
   "source": [
    "## Learning Objectives\n",
    "\n",
    "- Using Numba decorators to speed up algorithms targeting GPUs\n",
    "- Creating complex alogrithms utilizing Numba decorators targeting GPUs\n",
    "- Memory management with _CUDA_ kernels\n",
    "- Exploring General _CUDA_ kernels\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hMU9pYvQJVbq"
   },
   "source": [
    "We can check that our installation worked by trying to import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sPild-HfJVbs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dRApbp_zJVbu"
   },
   "source": [
    "## Problem 1) Exploring a basic algorithm targeting the GPU\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w1GFBEnNJVb8"
   },
   "source": [
    "We want to begin by compaaring a native numpy function which will leverage the CPU and compare that to a Ufunc that will target the CPU. \n",
    "\n",
    "Lets start by considering the addition of two numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T4UTxhKsJVb_"
   },
   "source": [
    "**Problem 1a** Run the following cell to compute the addition of two arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "DqcpLS9BJVcM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 22, 33, 44])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([10, 20, 30, 40])\n",
    "\n",
    "np.add(a, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_KDgS8TLIIE"
   },
   "source": [
    "More information on Numpy Ufuncs can be found [here](https://docs.scipy.org/doc/numpy/reference/ufuncs.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HigqTW-4LTRB"
   },
   "source": [
    "Next, we want to explore using Numba to create Ufuncs that target the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZL_tQqRILk8_"
   },
   "source": [
    "**Problem 1b** Use the `vectorize` decorator in Numba to write a function that adds to arrays. Use the `int64` data types and `target='cuda'`. Note: An overview including some common terminology for CUDA programming can be found [here](https://numba.readthedocs.io/en/stable/cuda/overview.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "_iSVUWE-MeFD",
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NvvmSupportError",
     "evalue": "libNVVM cannot be found. Do `conda install cudatoolkit`:\ndlopen(libnvvm.dylib, 0x0006): tried: '/Users/meh/miniforge3/envs/hpc/lib/python3.10/lib-dynload/../../libnvvm.dylib' (no such file), '/Users/meh/miniforge3/envs/hpc/bin/../lib/libnvvm.dylib' (no such file), 'libnvvm.dylib' (no such file), '/usr/local/lib/libnvvm.dylib' (no such file), '/usr/lib/libnvvm.dylib' (no such file), '/Users/meh/Desktop/programs/lsstc_dsfp/LSSTC-DSFP-Sessions/Sessions/Session15/Day5/libnvvm.dylib' (no such file)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "File \u001b[0;32m~/miniforge3/envs/hpc/lib/python3.10/site-packages/numba/cuda/cudadrv/nvvm.py:126\u001b[0m, in \u001b[0;36mNVVM.__new__\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 126\u001b[0m     inst\u001b[38;5;241m.\u001b[39mdriver \u001b[38;5;241m=\u001b[39m \u001b[43mopen_cudalib\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnvvm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniforge3/envs/hpc/lib/python3.10/site-packages/numba/cuda/cudadrv/libs.py:60\u001b[0m, in \u001b[0;36mopen_cudalib\u001b[0;34m(lib)\u001b[0m\n\u001b[1;32m     59\u001b[0m path \u001b[38;5;241m=\u001b[39m get_cudalib(lib)\n\u001b[0;32m---> 60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCDLL\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/hpc/lib/python3.10/ctypes/__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: dlopen(libnvvm.dylib, 0x0006): tried: '/Users/meh/miniforge3/envs/hpc/lib/python3.10/lib-dynload/../../libnvvm.dylib' (no such file), '/Users/meh/miniforge3/envs/hpc/bin/../lib/libnvvm.dylib' (no such file), 'libnvvm.dylib' (no such file), '/usr/local/lib/libnvvm.dylib' (no such file), '/usr/lib/libnvvm.dylib' (no such file), '/Users/meh/Desktop/programs/lsstc_dsfp/LSSTC-DSFP-Sessions/Sessions/Session15/Day5/libnvvm.dylib' (no such file)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNvvmSupportError\u001b[0m                          Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m vectorize \u001b[38;5;66;03m# complete\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;129;43m@vectorize\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mint64(int64, int64)\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# complete\u001b[39;49;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;43;01mdef\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;21;43madd_ufunc\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mreturn\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/hpc/lib/python3.10/site-packages/numba/np/ufunc/decorators.py:125\u001b[0m, in \u001b[0;36mvectorize.<locals>.wrap\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m    123\u001b[0m vec \u001b[38;5;241m=\u001b[39m Vectorize(func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkws)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sig \u001b[38;5;129;01min\u001b[39;00m ftylist:\n\u001b[0;32m--> 125\u001b[0m     \u001b[43mvec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43msig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ftylist) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    127\u001b[0m     vec\u001b[38;5;241m.\u001b[39mdisable_compile()\n",
      "File \u001b[0;32m~/miniforge3/envs/hpc/lib/python3.10/site-packages/numba/np/ufunc/deviceufunc.py:397\u001b[0m, in \u001b[0;36mDeviceVectorize.add\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    394\u001b[0m funcname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpyfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m    395\u001b[0m kernelsource \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_kernel_source(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kernel_template,\n\u001b[1;32m    396\u001b[0m                                        devfnsig, funcname)\n\u001b[0;32m--> 397\u001b[0m corefn, return_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevfnsig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m glbl \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_globals(corefn)\n\u001b[1;32m    399\u001b[0m sig \u001b[38;5;241m=\u001b[39m signature(types\u001b[38;5;241m.\u001b[39mvoid, \u001b[38;5;241m*\u001b[39m([a[:] \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args] \u001b[38;5;241m+\u001b[39m [return_type[:]]))\n",
      "File \u001b[0;32m~/miniforge3/envs/hpc/lib/python3.10/site-packages/numba/cuda/vectorizers.py:15\u001b[0m, in \u001b[0;36mCUDAVectorize._compile_core\u001b[0;34m(self, sig)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_compile_core\u001b[39m(\u001b[38;5;28mself\u001b[39m, sig):\n\u001b[0;32m---> 15\u001b[0m     cudevfn \u001b[38;5;241m=\u001b[39m \u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpyfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cudevfn, cudevfn\u001b[38;5;241m.\u001b[39moverloads[sig\u001b[38;5;241m.\u001b[39margs]\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mreturn_type\n",
      "File \u001b[0;32m~/miniforge3/envs/hpc/lib/python3.10/site-packages/numba/cuda/decorators.py:99\u001b[0m, in \u001b[0;36mjit.<locals>._jit\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m     97\u001b[0m targetoptions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastmath\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m fastmath\n\u001b[1;32m     98\u001b[0m targetoptions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m---> 99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mfunc_or_sig\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargetoptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargetoptions\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/hpc/lib/python3.10/site-packages/numba/cuda/compiler.py:805\u001b[0m, in \u001b[0;36mDispatcher.__init__\u001b[0;34m(self, py_func, sigs, targetoptions)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m targetoptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    804\u001b[0m     argtypes, restype \u001b[38;5;241m=\u001b[39m sigutils\u001b[38;5;241m.\u001b[39mnormalize_signature(sigs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 805\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43margtypes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    807\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompile(sigs[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/miniforge3/envs/hpc/lib/python3.10/site-packages/numba/cuda/compiler.py:1059\u001b[0m, in \u001b[0;36mDispatcher.compile_device\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m   1052\u001b[0m inline \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargetoptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1054\u001b[0m nvvm_options \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   1055\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdebug\u001b[39m\u001b[38;5;124m'\u001b[39m: debug,\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopt\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargetoptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mopt\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1057\u001b[0m }\n\u001b[0;32m-> 1059\u001b[0m cres \u001b[38;5;241m=\u001b[39m \u001b[43mcompile_cuda\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpy_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[43m                    \u001b[49m\u001b[43minline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnvvm_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnvvm_options\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverloads[args] \u001b[38;5;241m=\u001b[39m cres\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;66;03m# The inserted function uses the id of the CompileResult as a key,\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[38;5;66;03m# consistent with get_overload() above.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/hpc/lib/python3.10/site-packages/numba/core/compiler_lock.py:35\u001b[0m, in \u001b[0;36m_CompilerLock.__call__.<locals>._acquire_compile_lock\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire_compile_lock\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/hpc/lib/python3.10/site-packages/numba/cuda/compiler.py:202\u001b[0m, in \u001b[0;36mcompile_cuda\u001b[0;34m(pyfunc, return_type, args, debug, lineinfo, inline, fastmath, nvvm_options)\u001b[0m\n\u001b[1;32m    199\u001b[0m     flags\u001b[38;5;241m.\u001b[39mnvvm_options \u001b[38;5;241m=\u001b[39m nvvm_options\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# Run compilation pipeline\u001b[39;00m\n\u001b[0;32m--> 202\u001b[0m cres \u001b[38;5;241m=\u001b[39m \u001b[43mcompiler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_extra\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtypingctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypingctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mtargetctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtargetctx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpyfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m                              \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mreturn_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m                              \u001b[49m\u001b[38;5;28;43mlocals\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    209\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mpipeline_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCUDACompiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    211\u001b[0m library \u001b[38;5;241m=\u001b[39m cres\u001b[38;5;241m.\u001b[39mlibrary\n\u001b[1;32m    212\u001b[0m library\u001b[38;5;241m.\u001b[39mfinalize()\n",
      "File \u001b[0;32m~/miniforge3/envs/hpc/lib/python3.10/site-packages/numba/core/compiler.py:693\u001b[0m, in \u001b[0;36mcompile_extra\u001b[0;34m(typingctx, targetctx, func, args, return_type, flags, locals, library, pipeline_class)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[38;5;124;03m\"\"\"Compiler entry point\u001b[39;00m\n\u001b[1;32m    670\u001b[0m \n\u001b[1;32m    671\u001b[0m \u001b[38;5;124;03mParameter\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;124;03m    compiler pipeline\u001b[39;00m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    691\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m pipeline_class(typingctx, targetctx, library,\n\u001b[1;32m    692\u001b[0m                           args, return_type, flags, \u001b[38;5;28mlocals\u001b[39m)\n\u001b[0;32m--> 693\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_extra\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/hpc/lib/python3.10/site-packages/numba/core/compiler.py:429\u001b[0m, in \u001b[0;36mCompilerBase.compile_extra\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlifted \u001b[38;5;241m=\u001b[39m ()\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mlifted_from \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_bytecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/hpc/lib/python3.10/site-packages/numba/core/compiler.py:497\u001b[0m, in \u001b[0;36mCompilerBase._compile_bytecode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;124;03mPopulate and run pipeline for bytecode input\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfunc_ir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_core\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/hpc/lib/python3.10/site-packages/numba/core/compiler.py:476\u001b[0m, in \u001b[0;36mCompilerBase._compile_core\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;241m.\u001b[39mfail_reason \u001b[38;5;241m=\u001b[39m e\n\u001b[1;32m    475\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m is_final_pipeline:\n\u001b[0;32m--> 476\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CompilerError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll available pipelines exhausted\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/hpc/lib/python3.10/site-packages/numba/core/compiler.py:463\u001b[0m, in \u001b[0;36mCompilerBase._compile_core\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    461\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 463\u001b[0m     \u001b[43mpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mcr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    465\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/hpc/lib/python3.10/site-packages/numba/core/compiler_machinery.py:353\u001b[0m, in \u001b[0;36mPassManager.run\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    350\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed in \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m mode pipeline (step: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \\\n\u001b[1;32m    351\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline_name, pass_desc)\n\u001b[1;32m    352\u001b[0m patched_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_patch_error(msg, e)\n\u001b[0;32m--> 353\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m patched_exception\n",
      "File \u001b[0;32m~/miniforge3/envs/hpc/lib/python3.10/site-packages/numba/core/compiler_machinery.py:341\u001b[0m, in \u001b[0;36mPassManager.run\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    339\u001b[0m pass_inst \u001b[38;5;241m=\u001b[39m _pass_registry\u001b[38;5;241m.\u001b[39mget(pss)\u001b[38;5;241m.\u001b[39mpass_inst\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pass_inst, CompilerPass):\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_runPass\u001b[49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpass_inst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLegacy pass in use\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/hpc/lib/python3.10/site-packages/numba/core/compiler_lock.py:35\u001b[0m, in \u001b[0;36m_CompilerLock.__call__.<locals>._acquire_compile_lock\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_acquire_compile_lock\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m---> 35\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/hpc/lib/python3.10/site-packages/numba/core/compiler_machinery.py:296\u001b[0m, in \u001b[0;36mPassManager._runPass\u001b[0;34m(self, index, pss, internal_state)\u001b[0m\n\u001b[1;32m    294\u001b[0m     mutated \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m check(pss\u001b[38;5;241m.\u001b[39mrun_initialization, internal_state)\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SimpleTimer() \u001b[38;5;28;01mas\u001b[39;00m pass_time:\n\u001b[0;32m--> 296\u001b[0m     mutated \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mcheck\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pass\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minternal_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SimpleTimer() \u001b[38;5;28;01mas\u001b[39;00m finalize_time:\n\u001b[1;32m    298\u001b[0m     mutated \u001b[38;5;241m|\u001b[39m\u001b[38;5;241m=\u001b[39m check(pss\u001b[38;5;241m.\u001b[39mrun_finalizer, internal_state)\n",
      "File \u001b[0;32m~/miniforge3/envs/hpc/lib/python3.10/site-packages/numba/core/compiler_machinery.py:269\u001b[0m, in \u001b[0;36mPassManager._runPass.<locals>.check\u001b[0;34m(func, compiler_state)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck\u001b[39m(func, compiler_state):\n\u001b[0;32m--> 269\u001b[0m     mangled \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompiler_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mangled \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    271\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompilerPass implementations should return True/False. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    272\u001b[0m                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompilerPass with name \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m did not.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/hpc/lib/python3.10/site-packages/numba/cuda/compiler.py:118\u001b[0m, in \u001b[0;36mCUDALegalization.run_pass\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_pass\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;66;03m# Early return if NVVM 7\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnumba\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcudadrv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnvvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NVVM\n\u001b[0;32m--> 118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mNVVM\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mis_nvvm70:\n\u001b[1;32m    119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# NVVM < 7, need to check for charseq\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/hpc/lib/python3.10/site-packages/numba/cuda/cudadrv/nvvm.py:131\u001b[0m, in \u001b[0;36mNVVM.__new__\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__INSTANCE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    129\u001b[0m     errmsg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibNVVM cannot be found. Do `conda install \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    130\u001b[0m               \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcudatoolkit`:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 131\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NvvmSupportError(errmsg \u001b[38;5;241m%\u001b[39m e)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Find & populate functions\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, proto \u001b[38;5;129;01min\u001b[39;00m inst\u001b[38;5;241m.\u001b[39m_PROTOTYPES\u001b[38;5;241m.\u001b[39mitems():\n",
      "\u001b[0;31mNvvmSupportError\u001b[0m: libNVVM cannot be found. Do `conda install cudatoolkit`:\ndlopen(libnvvm.dylib, 0x0006): tried: '/Users/meh/miniforge3/envs/hpc/lib/python3.10/lib-dynload/../../libnvvm.dylib' (no such file), '/Users/meh/miniforge3/envs/hpc/bin/../lib/libnvvm.dylib' (no such file), 'libnvvm.dylib' (no such file), '/usr/local/lib/libnvvm.dylib' (no such file), '/usr/lib/libnvvm.dylib' (no such file), '/Users/meh/Desktop/programs/lsstc_dsfp/LSSTC-DSFP-Sessions/Sessions/Session15/Day5/libnvvm.dylib' (no such file)"
     ]
    }
   ],
   "source": [
    "from numba import vectorize # complete\n",
    "\n",
    "@vectorize(['int64(int64, int64)'], target='cuda') # complete\n",
    "def add_ufunc(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byv3W6puNqAC"
   },
   "source": [
    "Before running we need to be sure we have the hardware we would like to target!\n",
    "\n",
    "First, you'll need to enable GPUs for the notebook:\n",
    "\n",
    "- Navigate to Edit→Notebook Settings\n",
    "- select GPU from the Hardware Accelerator drop-down\n",
    "\n",
    "Check that the GPU was found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "B240Pi_UN34I"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "    raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mu9dqxDfOruh"
   },
   "source": [
    "More information can be found about the available types of devices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hk8mU8y7OXUh"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s39l-vQ2MpBz"
   },
   "source": [
    "**Problem 1c** Run your Ufunc which utilizes the GPU and compare the numerical result to Numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yfoLLhlkMoH8"
   },
   "outputs": [],
   "source": [
    "# complete\n",
    "assert np.allclose(# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ISBs9a8IPbN-"
   },
   "source": [
    "Now, lets use our favorite `timeit` magic command to see how much we benefitted from targeting the GPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zHdNjXQKJVcP"
   },
   "source": [
    "**Problem 1d** Use `timeit` to compare the execution time of the default Numpy Ufunc and our new Numba Ufunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ty9hOhdFJVcc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# complete   # NumPy on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHyk6SEnP2LG"
   },
   "outputs": [],
   "source": [
    "# complete # Numba on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "it4I4hqrQQfw"
   },
   "source": [
    "The GPU result is... *slower*?\n",
    "\n",
    "**Problem 1e** Discuss in this situation why our GPU result may be slower and how we can modify the problem to benfit fromm the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8j-yapHRQiVU"
   },
   "source": [
    "**Answer** \n",
    "\n",
    "Some points to consider: \n",
    "\n",
    " - **Our inputs are too small**: the GPU achieves performance through parallelism, operating on thousands of values at once. Our test inputs have only 4 and 16 integers, respectively. We need a much larger array to even keep the GPU busy.\n",
    "\n",
    "\n",
    "- **Our calculation is too simple**: Sending a calculation to the GPU involves quite a bit of overhead compared to calling a function on the CPU. If our calculation does not involve enough math operations (often called \"arithmetic intensity\"), then the GPU will spend most of its time waiting for data to move around.\n",
    "\n",
    "\n",
    "- **We copy the data to and from the GPU**: While including the copy time can be realistic for a single function, often we want to run several GPU operations in sequence. In those cases, it makes sense to send data to the GPU and keep it there until all of our processing is complete.\n",
    "\n",
    "\n",
    "- **Our data types are larger than necessary**: Our example uses int64 when we probably don't need it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q8Pip2rbRoEj"
   },
   "source": [
    "## Problem 2) Exploring a more complex, data-intensive algorithm targeting the GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2iy_vIiXRzA7"
   },
   "source": [
    "We will now consider a more complex example where we can take some of the necessary steps to make the problem more efficient to run on GPUs. A few of these steps include: \n",
    "\n",
    "1. Using native math module functions described [here](https://docs.python.org/3/library/math.html).\n",
    "2. Using a less precise datatype than necessary. Consider using `float32` instead. \n",
    "3. Solving a more complex algorithm - one with more math operations in this case than the addition of two arrays. \n",
    "4. Precompute constant values when possible.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olNN1JTwTFNi"
   },
   "source": [
    "**Problem 2a** Take the above steps to define the Ufunc for a Gaussian PDF using Numba `vectorize` again targeting `cuda`: \n",
    "\n",
    " $f(x) = \\frac{1}{\\sigma \\sqrt{2\\pi}} e^{-\\frac{1}{2} \\left ( \\frac{x-\\mu}{\\sigma} \\right )^{2}}$.\n",
    "\n",
    " Information on solving Normal distributions are discussed [here](https://en.wikipedia.org/wiki/Normal_distribution).[link text](https://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VYyEI4LrRx5L"
   },
   "outputs": [],
   "source": [
    "import math  # Note that for the CUDA target, we need to use the scalar functions from the math module, not NumPy\n",
    "\n",
    "SQRT_2PI = # complete  # Precompute this constant as a float32.  Numba will inline it at compile time.\n",
    "\n",
    "# complete\n",
    "def gaussian_pdf(x, mean, sigma):\n",
    "    '''Compute the value of a Gaussian probability density function at x with given mean and sigma.'''\n",
    "    return math.exp(-0.5 * ((x - mean) / sigma)**2) / (sigma * SQRT_2PI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8uM5sPSUR0R"
   },
   "source": [
    "**Problem 2b** Evaluate our Ufunc a million times, set $\\mu=0$ and $\\sigma=1$. Use `np.random.uniform` to create our `x` array for a bound of [-3,3].: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxNO39mtUSfx"
   },
   "outputs": [],
   "source": [
    "# Evaluate the Gaussian a million times!\n",
    "x = np.random.uniform# complete\n",
    "mean = # complete\n",
    "sigma =# complete\n",
    "\n",
    "# Quick test\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MuyzndFdU5sZ"
   },
   "source": [
    "**Problem 2c** Perform the same calculation using scipys native `norm` function (details [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html)). Time the results using `timeit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JuTYq6gjUz2I"
   },
   "outputs": [],
   "source": [
    "import scipy.stats # for definition of gaussian distribution\n",
    "norm_pdf = # complete\n",
    "%timeit # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7JZNpAwVVpE"
   },
   "source": [
    "**Problem 2d** Time the result for our GPU Ufunc for comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0J0hlysUVWN6"
   },
   "outputs": [],
   "source": [
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0FrZT5VVlgG"
   },
   "source": [
    "**Problem 2e** Thats a big improvement! \n",
    "\n",
    "Discuss some of the overhead costs still associated with this approach.\n",
    "\n",
    "**Answer**: Copying data to and from the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_iOIEzs_Wgsj"
   },
   "source": [
    "## Problem 3) Memory Management "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rsQax-fPaWwL"
   },
   "source": [
    "Some key terms before proceeding:\n",
    "\n",
    "- host: the CPU\n",
    "\n",
    "- device: the GPU\n",
    "\n",
    "- host memory: the system main memory\n",
    "\n",
    "- device memory: onboard memory on a GPU card\n",
    "\n",
    "- kernels: a GPU function launched by the host and executed on the device\n",
    "\n",
    "- device function: a GPU function executed on the device which can only be called from the device (i.e. from a kernel or another device function)\n",
    "\n",
    "In this problem we are going to explore explicit memory management as a way to further increase the speed of our algorithms. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ep5xoWkQdY_H"
   },
   "source": [
    "**Problem 3a** Use the `vectorize` decorator in Numba to write a function that adds two arrays. \n",
    "\n",
    "Use the `float32` data types and `target='cuda'`. \n",
    "\n",
    "This will serve as the basis of our timing analysis before optimization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HM5LkRnaaRx8"
   },
   "outputs": [],
   "source": [
    "# complete\n",
    "def add_ufunc(x, y):\n",
    "    return x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WkqOJZFIaVcr"
   },
   "source": [
    "**Problem 3b** Construct an array `x` of a range of 100000 values. Create an array `y=2*x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7U3C1Qg1aYAN"
   },
   "outputs": [],
   "source": [
    "n = 100000\n",
    "x = np.arange(# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLngR4PzacsD"
   },
   "source": [
    "**Problem 3c** Use these arrays to run and time (using `timeit` our baseline CUDA function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oi63pz1vaddC"
   },
   "outputs": [],
   "source": [
    "# complete # Baseline performance with host arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M1QeQABkboKX"
   },
   "source": [
    "Next, we want to utilize native Numba tools to copy the arrays we created _to device_ (to the GPU - recall our teriminology cheat sheet from earlier). Details about Memory Management using Numba CUDA is found [here](https://numba.readthedocs.io/en/stable/cuda/memory.html).\n",
    "\n",
    "\n",
    "**Problem 3d** Use Numba to copy our `x` and `y` arrays to device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U83bH98vbose"
   },
   "outputs": [],
   "source": [
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JNu74Slb02c"
   },
   "source": [
    "**Problem 3e** Call our `add_ufunc` routine using the _on device_ arrays and measure the execution speed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxvVIc95b1U0"
   },
   "outputs": [],
   "source": [
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sh0rmDmhb7ou"
   },
   "source": [
    "**Problem 3f** What is the main reason for the speedup observed here?\n",
    "\n",
    "**Answer:** We removed a key step from the process in our original call of `add_ufunc` as done in `Problem 3c`. We copied the array to device outside of the function call, thus removing the step from the calculation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NeQUEL5hg44R"
   },
   "source": [
    "With our current optimization, there is still an array created in our call to `add_ufunc` that then needs to be copied _back to host_ (the CPU). We can create the output array ahead of time and set it in our call using _device arrays_.\n",
    "\n",
    "**Problem 3g** Create an output array on device using Numba."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6HdWfCEhb8JK"
   },
   "outputs": [],
   "source": [
    "# complete  # does not initialize the contents, like np.empty()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZAiN8G_b_ZC"
   },
   "source": [
    "**Problem 3h** Finally, call our function passing all input and output device arrays we have created and time the execution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8Gxray7b_yt"
   },
   "outputs": [],
   "source": [
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H0uRuCiUcEbA"
   },
   "source": [
    "**Problem 3i** Perform the last step by hand, copy the output of the device array to host and print the first 10 elements to verify the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "flKXjWiacE6L"
   },
   "outputs": [],
   "source": [
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-WXt2f9YymF"
   },
   "source": [
    "## Problem 4) Writing CUDA Kernels\n",
    "\n",
    "Some definitions:\n",
    "\n",
    "A **kernel** function is a GPU function that is meant to be called from CPU code (*). \n",
    "\n",
    "It gives it two fundamental characteristics:\n",
    "\n",
    "- kernels cannot explicitly return a value; all result data must be written to an array passed to the function (if computing a scalar, you will probably pass a one-element array);\n",
    "\n",
    "- kernels explicitly declare their thread hierarchy when called: i.e. the number of thread blocks and the number of threads per block (note that while a kernel is compiled once, it can be called multiple times with different block sizes or grid sizes).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "895tb_hqMFpu"
   },
   "source": [
    "**Problem 4a** Write a _CUDA_ kernel version of our add function.\n",
    "\n",
    "Details on the API is available [here]().."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OtaWbDwzjDFo"
   },
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "# complete\n",
    "def add_kernel(x, y, out):\n",
    "    tx = # complete # this is the unique thread ID within a 1D block\n",
    "    ty = # complete  # Similarly, this is the unique block ID within the 1D grid\n",
    "\n",
    "    block_size = # complete  # number of threads per block\n",
    "    grid_size = # complete    # number of blocks in the grid\n",
    "    \n",
    "    start = tx + ty * block_size\n",
    "    stride = block_size * grid_size\n",
    "\n",
    "    # assuming x and y inputs are same length\n",
    "    for i in range(start, x.shape[0], stride):\n",
    "        out[i] = x[i] + y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lxGRcSZbMrag"
   },
   "source": [
    "**Problem 4b** Call our new _CUDA_ kernal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hDubS9k3OCa0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 5000000\n",
    "x = np.arange(n).astype(np.float32)\n",
    "y = 2 * x\n",
    "out = np.empty_like(x)\n",
    "\n",
    "threads_per_block = 128\n",
    "blocks_per_grid = 30\n",
    "\n",
    "add_kernel[blocks_per_grid, threads_per_block]# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOWfCNaeOMaV"
   },
   "source": [
    "**Problem 4c** Simplify our add kernel using `cuda` grid tools:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_XjXGE85ONCe"
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def add_kernel(x, y, out):\n",
    "    start = cuda.grid# complete     # 1 = one dimensional thread grid, returns a single value\n",
    "    stride = cuda.gridsiz# complete # ditto\n",
    "\n",
    "    # assuming x and y inputs are same length\n",
    "    for i in range(start, x.shape[0], stride):\n",
    "        out[i] = x[i] + y[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wQm0ZJ0vOhoO"
   },
   "source": [
    "**Problem 4d** As before, create device arrays instead of the implicit copy that takes place when passing NumPy arrays to our GPU kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AEmjl39IOicc"
   },
   "outputs": [],
   "source": [
    "# complete\n",
    "# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AVzLZpWIOy-U"
   },
   "source": [
    "**Problem 4e** Perform timing for the kernel with and without the use of device arrays.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "det7GaQ-OyJn",
    "outputId": "482080dc-55c9-419f-f817-3d9d74b1e0f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.29 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "10 loops, best of 5: 29 ms per loop\n"
     ]
    }
   ],
   "source": [
    "# complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ur7DOM6uQcC0",
    "outputId": "e62aae38-6526-4473-9e15-42141b52f99d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 loops, best of 5: 5.21 ms per loop\n"
     ]
    }
   ],
   "source": [
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EtyLO1xpQpwr"
   },
   "source": [
    "Some comments about Kernel synchronization:\n",
    "\n",
    "\n",
    "- One extremely important caveat should be mentioned here: CUDA kernel execution is designed to be asynchronous with respect to the host program. This means that the kernel launch `add_kernel[blocks_per_grid, threads_per_block](x_device, y_device, out_device)` returns immediately, allowing the CPU to continue executing while the GPU works in the background. Only host<->device memory copies or an explicit synchronization call will force the CPU to wait until previously queued CUDA kernels are complete.\n",
    "\n",
    "When you pass host NumPy arrays to a CUDA kernel, Numba has to synchronize on your behalf, but if you pass device arrays, processing will continue. If you launch multiple kernels in sequence without any synchronization in between, they will be queued up to run sequentially by the driver, which is usually what you want. If you want to run multiple kernels on the GPU in parallel (sometimes a good idea, but beware of race conditions!), take a look at CUDA streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahWqEf_ERvIY"
   },
   "source": [
    "**Problem 4f** Perform timing for the kernel with and without the use of device arrays and with explicit versus implicit synchronization for memory copies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4WqqEPSaQqZ-",
    "outputId": "0cf7e9df-8fab-42a4-f075-aeb742940fbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 30.2 ms, sys: 0 ns, total: 30.2 ms\n",
      "Wall time: 32.2 ms\n"
     ]
    }
   ],
   "source": [
    "# CPU input/output arrays, implied synchronization for memory copies\n",
    "# complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3yj7k7vlQvqT",
    "outputId": "d83407c1-7f62-47f4-8429-9a6006089e88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 573 µs, total: 573 µs\n",
      "Wall time: 583 µs\n"
     ]
    }
   ],
   "source": [
    "# GPU input/output arrays, no synchronization (but force sync before and after)\n",
    "# complete\n",
    "# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6mh2FFO-Q0JL",
    "outputId": "a57bdf11-9c11-4da0-ede0-c7dfb8c66e89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.43 ms, sys: 0 ns, total: 2.43 ms\n",
      "Wall time: 1.68 ms\n"
     ]
    }
   ],
   "source": [
    "# GPU input/output arrays, include explicit synchronization in timing\n",
    "# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pa6u-kBCR83k"
   },
   "source": [
    "**Question:** What is observed? When should we be mindful to sync?\n",
    "\n",
    "**Answer:** Always be sure to synchronize with the GPU when benchmarking CUDA kernels!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ds3xczZIdmdO"
   },
   "source": [
    "\n",
    "## Problem 5) Matrix multiplication (Optional/Challenge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkhOQBarScY9"
   },
   "source": [
    "**Problem 5a**: Rewrite the following implication of matrix multiplication using shared memory. \n",
    "\n",
    "Details on shared memory usage in _CUDA_ is described [here](https://numba.readthedocs.io/en/stable/cuda/memory.html#cuda-shared-memory)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1TF10tOO03r"
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def matmul(A, B, C):\n",
    "    \"\"\"Perform square matrix multiplication of C = A * B.\"\"\"\n",
    "    i, j = cuda.grid(2)\n",
    "    if i < C.shape[0] and j < C.shape[1]:\n",
    "        tmp = 0.\n",
    "        for k in range(A.shape[1]):\n",
    "            tmp += A[i, k] * B[k, j]\n",
    "        C[i, j] = tmp"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "fields_intro_to_GPUs.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python [conda env:hpc]",
   "language": "python",
   "name": "conda-env-hpc-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
